{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUACK Workshop: PCA in R\n",
    "## Fall 2017\n",
    "## Samy Abdel-Ghaffar & David Bourgin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE RUN THIS NOW!! It will take a while and Sam will blab his mouth while this runs.\n",
    "install.packages(\"ade4\")\n",
    "install.packages(\"fields\")\n",
    "library(ade4)\n",
    "library(fields)\n",
    "library(corrplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation: Why PCA?\n",
    "\n",
    "*From wikipedia:*\n",
    "\n",
    ">Principal Components Analysis (more commonly known as PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.\n",
    "\n",
    "Ok, that's great and all, but as with most wikipedia definitions of statistical procedures, I don't really understand what this concise definition is telling me until I understand what the statistical procedure in question does from a more intuitive perspective.\n",
    "\n",
    "Perhaps a better way to understand PCA is to motivate the scenarios when one would want to use it, through examples. Once you see the powerful results of conducting PCA, it will hopefully be easier to understand exactly what it does, and how it does it. So let's begin..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Dimensionality (Compression)\n",
    "\n",
    "At it's core, PCA is a technique that reduces the dimensionality of data in a very useful way. This dimensionality reduction is what makes PCA useful in other ways that we'll see shortly, but sometimes dimensionality reduction is all that is desired.\n",
    "\n",
    "Genmoics research deales with huge datasets, ~3 billion DNA base pairs in the human genome (although subsets are usually analyzed), where each base pair can be considered a dimension across humans. In order to make their analyses manegable, many geneticits employ PCA to reduce the dimensionality.\n",
    "\n",
    "Another way to think about dimensionality reduction is that it compresses a data set, by finding a smaller number of dimensions that do a \"good-enough\" job of representing the original data. This type of compression is called lossy because some information is lost. A classic example of applied compression is .mp3 music files. The original music files are very large, and so to make downloading them manegeable (at least back in the dark ages of the early 2000s), the music was compressed into mp3 files. While mp3 files don't use PCA to do compression, the concept is the same.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing High-Dimensional Data\n",
    "\n",
    "Visualization is a key part of conceptualizing and communicating data structure and analytic results. We are often confronted with high-dimensional data however, which poses a visualization challenge for our feeble 3-dimensional minds. Fortunately there are many techniques for reducing the dimensionality of data into 2 or 3 dimensions, which can then be easily visualized.\n",
    "\n",
    "To illustrate this, let's take a simple example of 3D data that we'd like to visualize in 2 dimensions. Here is a scatter plot of some 3-dimensional data.\n",
    "\n",
    "<img src=\"images/PCA_Visualization1.png\" align=\"left\" style=\"height: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly this data lies on a plane at an angle. One way of visualizing this in 2D would be to just use 2 of the dimensions, which would look like this:\n",
    "\n",
    "<img src=\"images/PCA_Visualization2.png\" align=\"left\" style=\"height: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that throws away some information about the relative distances between the data points. A better solution is to use a dimensionality reduction technique like PCA (or Multi-Dimensional Scaline, aka MDS, which David will discuss later). This is what the 2D plot looks like using hte first 2 principial components (PCs) from a PCA.\n",
    "\n",
    "<img src=\"images/PCA_Visualization3.png\" align=\"left\" style=\"height: 300px;\"/>\n",
    "\n",
    "Much better! While we can't actually imagine high-dimensional data in a visual way, we can use the 3D to 2D data reduction case as an analogy for reducing data that is hundreds or thousands of dimension in size down to data that exists in 10s of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Latent Variables\n",
    "\n",
    "> In statistics, latent variables (as opposed to observable variables), are variables that are not directly observed but are rather inferred (through a mathematical model) from other variables that are observed (directly measured).\n",
    "\n",
    "By finding a subset of interesting dimensions, PCA finds latent variables that capture much of the variance in the original data. \n",
    "\n",
    "In this example, many variables concerning emotional, cognitive and behavioral factors are all affected by a latent variable, namely depression. Once could imagine administering a questionnaire that asks many of these sorts of questions, and using PCA to identify latent variables such as depression and anxiety.\n",
    "\n",
    "<img src=\"images/LatentVariable_Depression.jpg\" align=\"left\" style=\"height: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising Data\n",
    "\n",
    "PCA can be used for removing noise from a data. The idea is that by dropping the least interesting principal components from your data you are removing variance that is mostly noise, and so you increase the SNR of your data. Below is an example of using PCA to denoise the classic USPS digit images. The top row contains the original images, the middle shows those images with white Guassian noise added to them, and the bottom row shows the images reconstructed with the bottom PCs missing. Clearly the bottom row is less noisy than the middle row! \n",
    "\n",
    "<img src=\"images/PCA_Denoising_Numbers.png\" align=\"left\" style=\"height: 300px;\"/>\n",
    "\n",
    "This same principal is the basis for another statistical technique called *Principal Components Regression*, which simply applies PCA to the independent variables before running a linear regression. By removing noise through the application of PCA, this technique aims to reduce over-fitting to the data. More info can be found here: https://en.wikipedia.org/wiki/Principal_component_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Basics\n",
    "\n",
    "Now that we have some idea of what PCA can do, let's cover some background concepts and assumptions of PCA before diving into the algorithm and code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra Background Concepts\n",
    "\n",
    "### Vector (Linear) Spaces and Linear Subspaces\n",
    "Given a collection of vectors in $\\mathbb{R}^n$, the **span** of the set is the collection of all points which can be reached using just linear combinations of the vectors.\n",
    "\n",
    "<img src=\"images/vector_arithmetic.png\" align=\"left\" style=\"height: 200px;\"/>\n",
    "<img src=\"images/span3.png\" align=\"left\" style=\"height: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projections & Rotations\n",
    "A matrix can be used to map a vector $\\mathbf{x}$ into a new vector $\\mathbf{y}$ via the relation $\\mathbf{y} = \\mathbf{Mx}$. The transformation is linear.\n",
    "\n",
    "One particularly useful kind of transformation is one which _projects_ a vector onto another vector. We can achieve a projection using matrix multiplication, as this transformation is linear.\n",
    "\n",
    "**Big Idea**: _Any_ linear function can be expressed as a matrix.\n",
    "\n",
    "<img src=\"images/rotation.png\" align=\"left\" style=\"height: 200px;\"/>\n",
    "<img src=\"images/2D_to_1D_Projection.jpg\" align=\"left\" style=\"height: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basis Functions (Vectors)\n",
    "A **basis set** is the smallest collection of vectors in the vector space that are sufficient to produce all other points in the span.\n",
    "\n",
    "For any spanning set, there exist an infinite number of basis sets. While formally equivalent, some basis sets are more useful for particular problems. \n",
    "\n",
    "A common technique in linear algebra is the **change of basis operation**, which translates the representation of a set of points in one basis to a representation in another basis. PCA is a prominent example of this technique.\n",
    "\n",
    "<img src=\"images/basis1.png\" align=\"left\" style=\"height: 200px;\"/>\n",
    "<img src=\"images/pcabasis.png\" align=\"left\" style=\"height: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "\n",
    "When using any statistical procedure it is always important to know and respect the assumptions underlying the technique. Let's look at the assumptions behind PCA now.\n",
    "\n",
    "For a wonderful tutorial on PCA, and the source of these assumptions see: https://arxiv.org/pdf/1404.1100.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearity\n",
    "\n",
    "Linearity frames the problem as a change of basis. Several areas of research have explored how extending these notions to nonlinear regimes. In other words, we assume that the data we have are created from a weighted linear combination of some other variables.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large variances have important structure.\n",
    "\n",
    "This assumption also encompasses the belief that the data has a high signal-to-noise-ratio (SNR). Hence, principal components with larger associated variances represent interesting structure, while those with lower variances represent noise. Note that this is a strong, and sometimes, incorrect assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonal Dimensions\n",
    "\n",
    "This assumption provides an intuitive simplification that makes PCA soluble with linear algebra decomposition techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "As with all statistical techniques, PCA has it's limitations. The primary limitation is that is can only find relationships that are pairwise linear correlations between the variables. In other words, PCA cannot find higher order relationships between data, such as polynomial, explonential or logarithmic relationships (as we saw in the Swiss Roll manifold example above). Because multivariate Gaussian distributions can be fully explained using the mean (which is removed in PCA) and the covariance, PCA has an implicit assumption that the data you are using in your PCA is multivariate Gaussian.\n",
    "\n",
    "Here's a more in-depth explanation of why, for those interested:\n",
    "https://www.quora.com/Are-there-implicit-Gaussian-assumptions-in-the-use-of-PCA-principal-components-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA: A More Formal Definition\n",
    "\n",
    "Now that we've given somewhat of an intuition as to what PCA can be used for, and have some basic linear algebra concepts necessary, let's take a more formal look at what PCA does. As with most (if not all!) statistical techniques, PCA can be thought of from a geometric perspective as well as an algebraic perspective. First we'll touch on the geometric perspective, then get into the algebraic perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric Perspective: Projection Onto Lower Dimensional Manifold\n",
    "\n",
    "Perhaps the simplest way to think about PCA is that it finds a useful manifold (and the linear subspace that contains that manifold) of the vector space where the data lives. The subspace it finds is comprised of the top N principal components (PCs). The data is then projected onto that lower dimensional subspace. Below is an example showing data in a 3D space, that is projected into a 2D subspace.\n",
    "\n",
    "<img src=\"images/pca_manifold.png\" align=\"left\" style=\"height: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algebraic Perspective: Matrix Decomposition\n",
    "\n",
    "> In linear algebra, a matrix decomposition or matrix factorization is a factorization of a matrix into a product of matrices.\n",
    "\n",
    "Algebraically, PCA conducts some form of matrix decomposition in order to find a set of orthogonal basis vectors. Each of these vectors is a weighted linear combination of the original data variables, and thus can be used to reconstruct the original data, each with varying degrees of accuracy. \n",
    "\n",
    "source: http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/\n",
    "<img src=\"images/rank_one.png\" align=\"left\" style=\"height: 200px;\"/>\n",
    "<img src=\"images/pca_3.png\" align=\"left\" style=\"height: 400px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "The derivation of PCA begins by defining a cost function that is to be minimized (or maximized). This cost function can be thought of in two equivalent ways. For a nice formal explanation of why this is true, see section 18.1.1 of this tutorial: http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf\n",
    "\n",
    "<img src=\"images/pca_two_views.png\" align=\"left\" style=\"height: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, PCA is finding those orthogonal basis vectors that minimize the sum of squared projections of each data point onto each basis vector (or component). From this perspective, the first PC is minimizing the following function:\n",
    "<img src=\"images/Minimizing_ProjectionResiduals.png\" align=\"left\" style=\"height: 50px;\"/>\n",
    "\n",
    "<img src=\"images/PCA_Error.gif\" align=\"left\" style=\"height: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, PCA is finding those orthogonal basis vectors that account for the most variance, in decending order. From this perspective, the first PC is minimizing the following function:\n",
    "\n",
    "<img src=\"images/Maximizing_Variance.png\" align=\"left\" style=\"height: 50px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalue Decomposition of Covariance Matrix\n",
    "\n",
    "The first algorithm to conduct PCA consists of several steps built around an eigenvalue decomposition of the covariance matrix of the dataset. Eigenvalue decomposition, and why this algorithm works to minimize the cost function, are beyond the scope of today's workshop. If you'd like to dig into this topic further, see this excellent chapter on linear algebra, that explains both: http://www.deeplearningbook.org/contents/linear_algebra.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps for conducting PCA\n",
    "1. Demean (and scale when appropriate) data\n",
    "2. Create the covariance matrix\n",
    "3. Conduct eigenvalue decomposition of covariance matrix\n",
    "4. Normalize the eignevectors to unit length\n",
    "5. Create scores \n",
    "6. Reconstruct the original data (optional sanity check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a simple dataset containing the data from the 1988 men's olympic decathalon to practice doing PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the olympic decathalon dataset and have a look at it\n",
    "data(olympic)\n",
    "str(olympic$tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Demean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demean the data\n",
    "olympicMat = as.matrix(olympic$tab)\n",
    "olympicDemean = scale(olympicMat, center=TRUE, scale=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Create the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotCovMat <- function(data) {\n",
    "    redBlueColors = colorRampPalette(c('blue', 'white', 'red'), space='Lab')(100)\n",
    "    data_R <- apply(data, 2, rev)\n",
    "    image.plot(t(data_R), zlim=c(min(data),max(data)), axes=FALSE, col = redBlueColors)\n",
    "    image(t(data_R), zlim=c(min(data),max(data)), axes=FALSE, col = redBlueColors, add=TRUE)\n",
    "    axis(3, at=seq(0,1, length=ncol(data)), labels=colnames(data), lwd=0, pos=1.025)\n",
    "    axis(2, at=seq(1,0, length=ncol(data)), labels=rownames(data), lwd=0, pos=-0.025)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot size\n",
    "options(repr.plot.width = 7, repr.plot.height = 7)\n",
    "\n",
    "# create the covariance matrix\n",
    "olympicDemeanCov = cov(olympicDemean)\n",
    "#options(repr.plot.width = 2, repr.plot.height = 2, repr.plot.res = 300)\n",
    "plotCovMat(olympicDemeanCov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Conduct eigenvalue decomposition of covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot size\n",
    "options(repr.plot.width = 4, repr.plot.height = 4)\n",
    "\n",
    "# do the eigenvalue decomposition\n",
    "olympicEig = eigen(olympicDemeanCov)\n",
    "str(olympicEig)\n",
    "\n",
    "# calculate the variance explained per PC\n",
    "olympicPCVarEx = (olympicEig$values / sum(olympicEig$values)) * 100.0\n",
    "plot(olympicPCVarEx, type='b', xlab='Component #', ylab='% Variance Explained', main=\"Scree plot\", col = 'black', pch=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Normalize the eignevectors to unit length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot size\n",
    "options(repr.plot.width = 8.5, repr.plot.height = 8.5)\n",
    "\n",
    "# Normalize PCs to unit length\n",
    "olympicPCALoadings = olympicEig$vectors / norm(olympicEig$vectors, type=\"2\")\n",
    "\n",
    "rownames(olympicPCALoadings) = colnames(olympicMat)\n",
    "colnames(olympicPCALoadings) = c('PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10')\n",
    "\n",
    "# visualize the loadings\n",
    "plotCovMat(olympicPCALoadings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Create scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create scores\n",
    "olympicPCAScores = olympicDemean %*% olympicPCALoadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot size\n",
    "options(repr.plot.width = 6, repr.plot.height = 6)\n",
    "\n",
    "# set the original variables names to be the row names of PCA Loadings\n",
    "rownames(olympicPCALoadings) = colnames(olympicMat)\n",
    "\n",
    "# make a bilplot showing the scores and loadings for the first 2 PCs\n",
    "biplot(olympicPCAScores[,1:2], olympicPCALoadings[,1:2], xlab='PC1', ylab='PC2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the R PCA function\n",
    "\n",
    "We went through the steps of PCA to give you an idea of what's involved. Of course R has several functions that do PCA for you in one simple function call. Let's look at this now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the built-in R function that does PCA using eigenvalue decomposition of the covariance matrix\n",
    "olympicPCAEig = princomp(olympic$tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether the scores and loadings that we calculated are the same as those the R function calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use abs() because some of the PCs are flipped, which doesn't matter except that the scores will be flipped too \n",
    "all.equal(abs(olympicPCAScores), abs(olympicPCAEig$scores), check.attributes = FALSE, check.names=FALSE)\n",
    "all.equal(abs(olympicPCALoadings), abs(as.matrix(olympicPCAEig$loadings[,])), check.attributes = FALSE, check.names=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Reconstruct the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct the data\n",
    "reconstructedData = t(t(olympicPCAScores %*% t(olympicPCALoadings)) + colMeans(olympicMat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify this reconstructed data is the same as the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all.equal(reconstructedData, olympicMat, , check.attributes = FALSE, check.names=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation vs. Covariance\n",
    "\n",
    "When all of your data is on the same scale, then using the covariance works just fine. But when your data is on different scales, PCA will locate the dimensions with highest variance, and so the variable(s) on the largest scale will dominate the PCs. To overcome this, you can simply scale your data, generally by dividing the standard deviation of each variable. In other words, you would z-score your data instead of just demeaning your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now let's scale the the data instead of just demeaning it\n",
    "olympicScale = scale(olympicMat, center=TRUE, scale=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# and get the covariance matrix for the scaled data\n",
    "olympicScaleCov = cov(olympicScale) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot size\n",
    "options(repr.plot.width = 7, repr.plot.height = 7)\n",
    "\n",
    "# get the correlation matrix of the original data\n",
    "olympicCorr = cor(olympicMat)\n",
    "all.equal(olympicScaleCov, olympicCorr, , check.attributes = FALSE, check.names=FALSE)\n",
    "\n",
    "# plot the correlation matrix as we did the covariance matrix \n",
    "plotCovMat(olympicCorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create a much prettier plot of the correlation matrix, sorted using\n",
    "# hierarchical clustering\n",
    "redBlueColors = colorRampPalette(c('blue', 'white', 'red'), space='Lab')(100)\n",
    "corrplot(olympicCorr, order = \"hclust\", tl.col='black', tl.cex=.75, col=redBlueColors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's redo the eigenvalue decomposition and recreate the scores using the correlation matrix instead\n",
    "olympicCorrEig = eigen(olympicCorr)\n",
    "str(olympicCorrEig)\n",
    "\n",
    "# calculate the variance explained by dividing each eigenvalue by the sum of all the eigenvalues\n",
    "olympicCorrPCVarEx = (olympicCorrEig$values / sum(olympicCorrEig$values)) * 100.0\n",
    "\n",
    "# make the scree plot\n",
    "par(mfrow=c(1,2))\n",
    "plot(olympicPCVarEx, type='b', xlab='Component #', ylab='% Variance Explained', main=\"Scree plot Covariance\", pch=16)\n",
    "plot(olympicCorrPCVarEx, type='b', xlab='Component #', ylab='% Variance Explained', main=\"Scree plot Correlation\", pch=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot size\n",
    "options(repr.plot.width = 8.5, repr.plot.height = 8.5)\n",
    "\n",
    "# calculate the loadings by scaling the eigenvectors to unit lenght (i.e. 1)\n",
    "olympicCorrPCALoadings = olympicCorrEig$vectors / norm(olympicCorrEig$vectors, type=\"2\")\n",
    "\n",
    "rownames(olympicCorrPCALoadings) = colnames(olympicMat)\n",
    "colnames(olympicCorrPCALoadings) = c('PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10')\n",
    "\n",
    "# visualize the loadings\n",
    "plotCovMat(olympicCorrPCALoadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the scores by multiplying the scaled data with the loadings \n",
    "olympicCorrPCAScores = olympicScale %*% olympicCorrPCALoadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot size\n",
    "options(repr.plot.width = 6, repr.plot.height = 6)\n",
    "\n",
    "rownames(olympicCorrPCALoadings) = colnames(olympicMat)\n",
    "\n",
    "# make a bilplot showing the scores and loadings for the first 2 PCs\n",
    "biplot(olympicCorrPCAScores[,1:2], olympicCorrPCALoadings[,1:2], xlab='PC1', ylab='PC2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares vs. PCA\n",
    "\n",
    "We have learned that PCA is minimizing the squared error between the principal components (each a line) and the data. You have likely learned about another technique that minimzes the squared error of data, namely simple regression using ordinary least squares (OLS). PCA and OLS are somewhat similar techniques, although they differ in important ways. \n",
    "\n",
    "The first thing to note is that PCA is an unsupervised technique, meaning there is no data we are trying to predict (i.e. no dependent variable, or y). While OLS is a supervised technique that is trying to use a dataset (the independent variable(s), or x) to predict a second dataset (the dependent variable, or y). \n",
    "\n",
    "Perhaps the easiest way to see how they differ is to visualize the solutions, and the values they are miniming. Let's do that now by doing PCA and OLS on two of the variables from our olympic decathalon dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a couple helper functions to do projections onto a line.\n",
    "# source: https://stackoverflow.com/questions/8457279/visual-comparison-of-regression-pca\n",
    "pointOnLineNearPoint <- function(Px, Py, slope, intercept) {\n",
    "    # Px, Py is the point to test, can be a vector.\n",
    "    # slope, intercept is the line to check distance.\n",
    "    Ax <- Px-10*diff(range(Px))\n",
    "    Bx <- Px+10*diff(range(Px))\n",
    "    Ay <- Ax * slope + intercept\n",
    "    By <- Bx * slope + intercept\n",
    "    pointOnLine(Px, Py, Ax, Ay, Bx, By)\n",
    "    }\n",
    "\n",
    "pointOnLine <- function(Px, Py, Ax, Ay, Bx, By) {\n",
    "    # This approach based upon comingstorm's answer on\n",
    "    # stackoverflow.com/questions/3120357/get-closest-point-to-a-line\n",
    "    # Vectorized by Bryan\n",
    "    PB <- data.frame(x = Px - Bx, y = Py - By)\n",
    "    AB <- data.frame(x = Ax - Bx, y = Ay - By)\n",
    "    PB <- as.matrix(PB)\n",
    "    AB <- as.matrix(AB)\n",
    "    k_raw <- k <- c()\n",
    "    for (n in 1:nrow(PB)) {\n",
    "        k_raw[n] <- (PB[n,] %*% AB[n,])/(AB[n,] %*% AB[n,])\n",
    "        if (k_raw[n] < 0)  { k[n] <- 0\n",
    "            } else { if (k_raw[n] > 1) k[n] <- 1\n",
    "                else k[n] <- k_raw[n] }\n",
    "        }\n",
    "    x = (k * Ax + (1 - k)* Bx)\n",
    "    y = (k * Ay + (1 - k)* By)\n",
    "    ans <- data.frame(x, y)\n",
    "    ans\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subset the olympic dataset to get the times for the athletes in the 400m and 1500m races\n",
    "olympicSub = olympicMat[,c(\"400\",\"1500\")]\n",
    "olympicSub_Z = scale(olympicSub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot size\n",
    "options(repr.plot.width = 8, repr.plot.height = 4)\n",
    "\n",
    "####################### PCA #######################\n",
    "\n",
    "# calculate PCA on these two \n",
    "olympicSubPCA = prcomp(olympicSub_Z)\n",
    "\n",
    "# calculate the slope of the first PC, and we know the intercept is 0 because PCA is centered\n",
    "slopePC1 = olympicSubPCA$rotation[2,1] / olympicSubPCA$rotation[1,1]\n",
    "interceptPC1 = 0\n",
    "\n",
    "# plot the original data\n",
    "par(mfrow=c(1,2))\n",
    "#layout(matrix(c(1,2), 1, 2, byrow = TRUE), widths=c(1,1), heights=c(.5,.5))\n",
    "plot(scale(olympicSub), pch=16, main=\"PCA\")\n",
    "\n",
    "# plot the PC as a line\n",
    "abline(interceptPC1, slopePC1)\n",
    "\n",
    "# calculate the projections of the data onto the first PC, and plot those lines\n",
    "projectionsPCA <- pointOnLineNearPoint(olympicSub_Z[,1], olympicSub_Z[,2], slopePC1, interceptPC1)\n",
    "segments( olympicSub_Z[,1], olympicSub_Z[,2], projectionsPCA[,1], projectionsPCA[,2], col = \"blue\")\n",
    "\n",
    "####################### OLS #######################\n",
    "# calculate OLS on these two \n",
    "olympicSubOLS = lm(olympicSub_Z[,2]~olympicSub_Z[,1])\n",
    "\n",
    "# plot the original data\n",
    "plot(olympicSub_Z, pch=16, main=\"OLS\")\n",
    "\n",
    "# plot the regression line\n",
    "abline(olympicSubOLS$coefficients[1], olympicSubOLS$coefficients[2])\n",
    "\n",
    "# plot the residual lines\n",
    "segments(olympicSub_Z[,1], olympicSub_Z[,2], olympicSub_Z[,1], fitted(olympicSubOLS), col = \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD \n",
    "SVD is a second type of matrix decomposition that will give you PCs. Just for reference, there is a second R function that uses SVD instead of eigendecomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olympicPCASVD = prcomp(olympic$tab)\n",
    "all.equal(abs(olympicPCASVD$x), abs(olympicPCAEig$scores), check.attributes = FALSE, check.names=FALSE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
